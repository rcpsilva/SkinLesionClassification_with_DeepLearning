{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SkinLesionClassification_TransferLearningNET_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIUgYgYYsXnR",
        "colab_type": "text"
      },
      "source": [
        "# Skin lesion classification with transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGP95aMYsSKT",
        "colab_type": "text"
      },
      "source": [
        "Most of the code in this notebook was taken from https://github.com/falloutdurham/beginners-pytorch-deep-learning/. Here, it was tweaked to solve the sking lesion classification problem \n",
        "\n",
        "Please refer to the following book for more details:\n",
        "Programming PyTorch for Deep Learning by Ian Pointer - Released September 2019, Publisher(s): O'Reilly Media, Inc., ISBN: 9781492045342"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzEg1gylynMQ",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKI4YjNgfSbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "folder_path = \"/content/drive/My Drive/Skin_Lesions\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcjUvclazccd",
        "colab_type": "text"
      },
      "source": [
        "## Get data "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO3_CWfTznw1",
        "colab_type": "text"
      },
      "source": [
        "### Image transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enL5-qX0zfNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_size = 256\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    #transforms.RandomHorizontalFlip(p=0.5), # used for data augmentation\n",
        "    #transforms.RandomVerticalFlip(p=0.5), # used for data augmentation\n",
        "    transforms.Resize((im_size,im_size)), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225] )                            \n",
        "])\n",
        "\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.Resize((im_size,im_size)), \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                    std=[0.229, 0.224, 0.225] )\n",
        "    ])\n",
        "\n",
        "def check_image(path):\n",
        "    try:\n",
        "        im = Image.open(path)\n",
        "        return True\n",
        "    except:\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQRhKfPFC286",
        "colab_type": "text"
      },
      "source": [
        "## Class balancing sampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0MPozknCwY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_weights_for_balanced_classes(images, nclasses):                        \n",
        "    count = [0] * nclasses                                                      \n",
        "    for item in images:                                                         \n",
        "        count[item[1]] += 1                                                     \n",
        "    weight_per_class = [0.] * nclasses                                      \n",
        "    N = float(sum(count))                                                   \n",
        "    for i in range(nclasses):                                                   \n",
        "        weight_per_class[i] = N/float(count[i])                                 \n",
        "    weight = [0] * len(images)                                              \n",
        "    for idx, val in enumerate(images):                                          \n",
        "        weight[idx] = weight_per_class[val[1]]                                  \n",
        "    return weight "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEYjUjNdC8s7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_path = folder_path + \"/train/\"\n",
        "train_data = torchvision.datasets.ImageFolder(root=train_data_path, transform=train_transform, is_valid_file=check_image)                                                                        \n",
        "                                                                                \n",
        "# For unbalanced dataset we create a weighted sampler                       \n",
        "weights = make_weights_for_balanced_classes(train_data.imgs, len(train_data.classes))                                                                \n",
        "weights = torch.DoubleTensor(weights)                                       \n",
        "train_sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, num_samples=1000, replacement=True)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,sampler=train_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FlNf2-FzvEY",
        "colab_type": "text"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHPKy_AFzuD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_path = folder_path + \"/test/\"\n",
        "test_data = torchvision.datasets.ImageFolder(root=test_data_path, transform=img_transforms, is_valid_file=check_image)\n",
        "\n",
        "val_data_path = folder_path + \"/val/\"\n",
        "val_data = torchvision.datasets.ImageFolder(root=val_data_path, transform=img_transforms, is_valid_file=check_image)\n",
        "\n",
        "val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size=batch_size,shuffle=True)\n",
        "test_data_loader  = torch.utils.data.DataLoader(test_data, batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc6_QWJpzB2P",
        "colab_type": "text"
      },
      "source": [
        "## Load pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSbGCPC2zBSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transfer_model = models.resnet50(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi50CMRc0EW6",
        "colab_type": "text"
      },
      "source": [
        "## Freeze parameters\n",
        "\n",
        "Freezing everything except batch normalization layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMzpcdwyypoL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name, param in transfer_model.named_parameters():\n",
        "    if(\"bn\" not in name):\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SCqm4fx0inB",
        "colab_type": "text"
      },
      "source": [
        "## Replacing the classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGsJQCDS0k_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transfer_model.fc = nn.Sequential(nn.Linear(transfer_model.fc.in_features,500),\n",
        "nn.ReLU(),                                 \n",
        "nn.Dropout(), nn.Linear(500,2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdXOpEXO0yQr",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxiVHgKt0tdP",
        "colab_type": "text"
      },
      "source": [
        "### Learning Rate Finder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1iG6xLW0sxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0, device=\"cpu\"):\n",
        "    number_in_epoch = len(train_loader) - 1\n",
        "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
        "    lr = init_value\n",
        "    optimizer.param_groups[0][\"lr\"] = lr\n",
        "    best_loss = 0.0\n",
        "    batch_num = 0\n",
        "    losses = []\n",
        "    log_lrs = []\n",
        "    for data in train_loader:\n",
        "        batch_num += 1\n",
        "        inputs, targets = data\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        # Crash out if loss explodes\n",
        "\n",
        "        if batch_num > 1 and loss > 4 * best_loss:\n",
        "            if(len(log_lrs) > 20):\n",
        "                return log_lrs[10:-5], losses[10:-5]\n",
        "            else:\n",
        "                return log_lrs, losses\n",
        "\n",
        "        # Record the best loss\n",
        "\n",
        "        if loss < best_loss or batch_num == 1:\n",
        "            best_loss = loss\n",
        "\n",
        "        # Store the values\n",
        "        losses.append(loss.item())\n",
        "        log_lrs.append((lr))\n",
        "\n",
        "        # Do the backward pass and optimize\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the lr for the next step and store\n",
        "\n",
        "        lr *= update_step\n",
        "        optimizer.param_groups[0][\"lr\"] = lr\n",
        "    if(len(log_lrs) > 20):\n",
        "        return log_lrs[10:-5], losses[10:-5]\n",
        "    else:\n",
        "        return log_lrs, losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPbug8Oa04AK",
        "colab_type": "text"
      },
      "source": [
        "## Training procedure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy_MlPu_001J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
        "    for epoch in range(epochs):\n",
        "        training_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output = model(inputs)\n",
        "            loss = loss_fn(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss += loss.data.item() * inputs.size(0)\n",
        "        training_loss /= len(train_loader.dataset)\n",
        "        \n",
        "        model.eval()\n",
        "        num_correct = 0 \n",
        "        num_examples = 0\n",
        "        for batch in val_loader:\n",
        "            inputs, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            output = model(inputs)\n",
        "            targets = targets.to(device)\n",
        "            loss = loss_fn(output,targets) \n",
        "            valid_loss += loss.data.item() * inputs.size(0)\n",
        "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n",
        "            num_correct += torch.sum(correct).item()\n",
        "            num_examples += correct.shape[0]\n",
        "        valid_loss /= len(val_loader.dataset)\n",
        "\n",
        "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
        "        valid_loss, num_correct / num_examples))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKlq0Gth23JY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\") \n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE8O8GB71Fov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "c72b1de8-55d4-4a4f-dfce-7d650ff8ff10"
      },
      "source": [
        "transfer_model.to(device)\n",
        "optimizer = optim.Adam(transfer_model.parameters(), lr=0.001)\n",
        "\n",
        "(lrs, losses) = find_lr(transfer_model, torch.nn.CrossEntropyLoss(),optimizer, train_data_loader,device=device)\n",
        "plt.plot(lrs, losses)\n",
        "\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCiQMCYS2FdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "15bb8305-8aff-40d0-f150-f382e0563830"
      },
      "source": [
        "optimizer = optim.Adam(transfer_model.parameters(), lr=lrs)\n",
        "train(transfer_model, optimizer,torch.nn.CrossEntropyLoss(), train_data_loader,val_data_loader, epochs=15, device=device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Training Loss: 2.11, Validation Loss: 0.76, accuracy = 0.47\n",
            "Epoch: 1, Training Loss: 0.69, Validation Loss: 0.49, accuracy = 0.75\n",
            "Epoch: 2, Training Loss: 0.46, Validation Loss: 0.40, accuracy = 0.83\n",
            "Epoch: 3, Training Loss: 0.23, Validation Loss: 0.46, accuracy = 0.82\n",
            "Epoch: 4, Training Loss: 0.13, Validation Loss: 0.57, accuracy = 0.83\n",
            "Epoch: 5, Training Loss: 0.04, Validation Loss: 0.68, accuracy = 0.85\n",
            "Epoch: 6, Training Loss: 0.02, Validation Loss: 0.59, accuracy = 0.85\n",
            "Epoch: 7, Training Loss: 0.03, Validation Loss: 0.98, accuracy = 0.83\n",
            "Epoch: 8, Training Loss: 0.01, Validation Loss: 0.65, accuracy = 0.83\n",
            "Epoch: 9, Training Loss: 0.01, Validation Loss: 0.87, accuracy = 0.83\n",
            "Epoch: 10, Training Loss: 0.00, Validation Loss: 0.84, accuracy = 0.83\n",
            "Epoch: 11, Training Loss: 0.00, Validation Loss: 0.94, accuracy = 0.83\n",
            "Epoch: 12, Training Loss: 0.00, Validation Loss: 0.99, accuracy = 0.83\n",
            "Epoch: 13, Training Loss: 0.00, Validation Loss: 0.96, accuracy = 0.83\n",
            "Epoch: 14, Training Loss: 0.00, Validation Loss: 1.01, accuracy = 0.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKtqWz1L3GPQ",
        "colab_type": "text"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob0__WIs3Hax",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cce0b86f-67c5-48c1-b7a5-aeed4e39ac4d"
      },
      "source": [
        "model = transfer_model\n",
        "num_correct = 0 \n",
        "num_examples = 0\n",
        "valid_loss = 0.0\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "mag_total = 0\n",
        "mag_right = 0\n",
        "ben_total = 0\n",
        "ben_right = 0\n",
        "\n",
        "\n",
        "for batch in test_data_loader:\n",
        "  inputs, targets = batch\n",
        "  inputs = inputs.to(device)\n",
        "  output = model(inputs)\n",
        "\n",
        "  targets = targets.to(device)\n",
        "  loss = loss_fn(output,targets) \n",
        "  valid_loss += loss.data.item() * inputs.size(0)\n",
        "\n",
        "  classes = torch.max(F.softmax(output), dim=1)[1]\n",
        "  \n",
        "  for c,t in zip(classes,targets):\n",
        "    if t == 1:\n",
        "      mag_total += 1\n",
        "      if c == 1:\n",
        "        mag_right += 1\n",
        "    else:\n",
        "      ben_total += 1\n",
        "      if c == 0:\n",
        "         ben_right += 1\n",
        "\n",
        "  correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n",
        "  num_correct += torch.sum(correct).item()\n",
        "  num_examples += correct.shape[0]\n",
        "valid_loss /= len(test_data_loader.dataset)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcpHKAPM3iM4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "c11436c1-3e9c-4602-c711-2b059aaf9b5d"
      },
      "source": [
        "print('Test loss: ' + str(valid_loss) + ' Accuracy: ' + str(num_correct / num_examples))\n",
        "print('Acurracy malignant ' + str(mag_right/mag_total))\n",
        "print('Acurracy benign ' + str(ben_right/ben_total))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.1077609658241272 Accuracy: 0.852112676056338\n",
            "Acurracy malignant 0.42857142857142855\n",
            "Acurracy benign 0.956140350877193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDw1Gl5YNOXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}